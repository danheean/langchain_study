{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f2ab50-0a19-47eb-8d7e-64b8abe9d719",
   "metadata": {},
   "source": [
    "## Web scraping(웹 스크래핑, 문서 수집하기)\n",
    "[Open In Colab](https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.1/docs/docs/use_cases/web_scraping.ipynb)\n",
    "\n",
    "### 4. Research automation\n",
    "* 구글 검색을 활용한 리서치 자동화\n",
    "  * [Programmable Search Engine](https://cse.google.com/cse?cx=a61c425e10bee4b8d)\n",
    "* APIFY를 활용한 리서치 자동화\n",
    "  * [Integration Tutorial: How to use LangChain with Apify scrapers](https://www.youtube.com/watch?v=zcfeiVdiGJg)\n",
    "\n",
    "#### 구성도\n",
    "<img src=\"./images/web_research.png\" width=\"800\">\n",
    "\n",
    "- [langchain-ai/web-explorer](https://github.com/langchain-ai/web-explorer) 예시 화면\n",
    "\n",
    "<img src=\"./images/research_automation01.png\" width=\"600\"> <img src=\"./images/research_automation02.png\" width=\"522\">\n",
    "\n",
    "\n",
    "#### Resources\n",
    " - [Custom Search JSON API: 소개](https://developers.google.com/custom-search/v1/introduction?hl=k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea33bd0-2bb5-4730-86a7-aac76191639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Your Python version is 3.11.9 (main, May 28 2024, 08:05:11) [GCC 8.5.0 20210514 (Red Hat 8.5.0-20)]\n"
     ]
    }
   ],
   "source": [
    "from python_environment_check import check_packages\n",
    "\n",
    "lib = {\n",
    "    'streamlit': '1.29.0', \n",
    "    'langchain': '0.0.354',\n",
    "    'chromadb': '0.4.3', \n",
    "    'openap': '1.30.1',\n",
    "    'langchain-openai': '0.1.7',\n",
    "    'langchain-core': '0.2.0',\n",
    "    'langchain-chroma': '0.1.1',\n",
    "    'openai': '1.30.1',\n",
    "    'google-api-core': '2.11.1',\n",
    "    'google-api-python-client': '2.95.0',\n",
    "    'google-auth': '2.22.0',\n",
    "    'google-auth-httplib2': '0.1.0',\n",
    "    'googleapis-common-protos': '1.59.1',\n",
    "    'tiktoken': '0.7.0',\n",
    "    'faiss-cpu': '1.8.0',\n",
    "    'apify-client': '1.7.0', \n",
    "#    'beautifulsoup4': '4.12.2',\n",
    "    'html2text': '2024.2.26',\n",
    "    'dotenv': '0.0.5',\n",
    "    'nest_asyncio': '1.6.0',\n",
    "}\n",
    "\n",
    "#check_packages(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ef7a38-d028-4685-9a53-bdfe4d962901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv('../envls')\n",
    "\n",
    "# 미리 실행해야 함\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 미리 설치해야 함\n",
    "#!playwright install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05dc7ac-6863-48da-ad4d-bbb9d5cb4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.environ['GOOGLE_CSE_ID'])\n",
    "#print(os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "# 리눅스에서 sqlite3 인식\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066e9e2-5274-4e5f-a662-c42b06345dde",
   "metadata": {},
   "source": [
    "### 구글 검색을 활용한 리서치 자동화\n",
    "#### 구글 Custom Search Engine 생성 및 키 필요\n",
    "<img src=\"./images/googlecse01.png\" width=\"970\">\n",
    "<img src=\"./images/googlecse02.png\" width=\"800\">\n",
    "<img src=\"./images/googlecse03.png\" width=\"980\">\n",
    "<img src=\"./images/googlecse04.png\" width=\"800\">\n",
    "<img src=\"./images/googlecse05.png\" width=\"800\">\n",
    "<img src=\"./images/googlecse06.png\" width=\"880\">\n",
    "<img src=\"./images/googlecse07.png\" width=\"980\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32279b81-7c4e-4ab7-9c2d-3d839abe7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "#import chromadb\n",
    "#from langchain.indexes import VectorstoreIndexCreator\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96f3a56-9d07-496d-b14d-18bfe4dcd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/www/.pyenv/versions/3.11.9/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `GoogleSearchAPIWrapper` was deprecated in LangChain 0.0.33 and will be removed in 0.3.0. An updated version of the class exists in the langchain-google-community package and should be used instead. To use it run `pip install -U langchain-google-community` and import as `from langchain_google_community import GoogleSearchAPIWrapper`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Get the version of ChromaDB\n",
    "# chroma_version = \"0.4\"\n",
    "# chroma_version = chromadb.__version__\n",
    "persist_directory = \"./db/chroma_db_oai\"\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(), persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Search\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e30f677-bfe2-4aea-95c6-4c8d7229c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore, llm=llm, search=search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea875dca-fa2d-4115-8ccf-ded859370cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:00<00:00, 11.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '고려대학교의 학생수는 몇명인가?',\n",
       " 'answer': '고려대학교의 학생수는 서울캠퍼스에 30,320명이 있고, 세종캠퍼스에 7,501명이 있습니다. 따라서 총 재학생 수는 37,821명입니다.\\n',\n",
       " 'sources': 'https://www.korea.ac.kr/mbshome/mbs/university/subview.do?id=university_010106000000, https://ko.wikipedia.org/wiki/%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.DEBUG)\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "#user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "#user_input = \"고래대학교의 학생수는?\"\n",
    "# user_input = \"고려대학교의 위치는 어디인가?\"\n",
    "user_input = \"고려대학교의 학생수는 몇명인가?\"\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm, retriever=web_research_retriever\n",
    ")\n",
    "result = qa_chain({\"question\": user_input})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aefb8-f67d-47f5-b44a-32936afb5639",
   "metadata": {},
   "source": [
    "#### * APIFY를 활용한 리서치 자동화\n",
    "#### [APIFY](https://apify.com/)\n",
    "* 2015년에 설립된 프라하 기반 스타트업\n",
    "* 대량의 웹 데이터를 수집 및 분석하고 웹 프로세스를 자동화하는 오픈 소스 도구 제공\n",
    "* Apify Store를 운영하며, 필요한 스크래퍼를 등록하여 사용할 수 있음\n",
    "\n",
    "* Apify 회원 가입 후 Apify Store에서 Actor id 저장\n",
    "  * 해당 Actor의 scheme을 확인하여 검색하고자 하는 속성명 저장\n",
    "  * 아래 1. 수집기를 활용한 방법에 적용한 후 실행\n",
    "* Actor을 구동하면 Storage에 데이터셋이 저장되어 있음\n",
    "  * 데이터셋 아이디가 있으면, 2. 수집한 데이터셋을 활용한 방법으로 진행해도 됨\n",
    "\n",
    "<img src=\"./images/apify00.png\" width=\"700\">\n",
    "<img src=\"./images/apify01.png\" width=\"1050\">\n",
    "<img src=\"./images/apify05.png\" width=\"980\">\n",
    "<img src=\"./images/apify02.png\" width=\"800\">\n",
    "<img src=\"./images/apify03.png\" width=\"800\">\n",
    "<img src=\"./images/apify04.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8579dfe3-d9f6-4b42-8761-a95e38d9314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/www/.pyenv/versions/3.11.9/envs/langchain/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:129: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection은 제품, 시설, 문서 등을 세밀하게 조사하거나 검토하는 과정을 의미합니다. 이 과정은 일반적으로 제품의 품질이나 안전성을 확인하거나 규정에 따른 규정 준수 여부를 평가하기 위해 수행됩니다. Inspection은 주로 시각적으로 이루어지지만 필요에 따라 다양한 도구나 장비를 사용하여 수행될 수도 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_community.utilities import ApifyWrapper\n",
    "from langchain_community.document_loaders import ApifyDatasetLoader\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "# Call the Actor to obtain text from the crawled webpages\n",
    "\"\"\"\n",
    "1. 수집기를 활용한 방법\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    #actor_id=\"astronomical_lizard/naver-blog-scraper\", \n",
    "    #run_input={\"startUrls\": [{\"url\": \"/docs/integrations/chat/\"}]},\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://docs.apify.com/academy/web-scraping-for-beginners\"}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"full_text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\"\"\"\n",
    "# 2. 수집한 데이터셋을 활용한 방법\n",
    "loader = ApifyDatasetLoader(\n",
    "    #dataset_id=\"mCkg6zDcTEO5JYX2F\", \n",
    "    dataset_id=\"rneQUybW8o0bUwZpK\", \n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"full_text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a vector store based on the crawled data\n",
    "index = VectorstoreIndexCreator(embedding=OpenAIEmbeddings()).from_loaders([loader])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=\"0\")\n",
    "\n",
    "# Query the vector store\n",
    "#query = \"Are any OpenAI chat models integrated in LangChain?\"\n",
    "query = \"Inspection에 대해서 알려주세요\"\n",
    "result = index.query(query, llm=llm)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddae491-3eaf-425d-8e58-cbc22726cdfb",
   "metadata": {},
   "source": [
    "## 결론\n",
    "- 서비스하기에는 속도가 느리다. (개선할 부분을 찾는 것이 숙제)\n",
    "- 제공되는 Transformer의 세부적인 옵션이 필요하다.\n",
    "  - BeautifulSoup 라이브러리를 사용하는 것이 더 효과적일 수 있다.\n",
    "- 상용 검색엔진과 연동하면 더 효과적으로 서비스할 수 있을 것 같다.\n",
    "- 스크래퍼의 변환 결과와 LLM의 스키마 및 프롬프트 결과를 위한 충분한 테스트 필요\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 64-bit ('langchain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1e6c2cf2a7c8a2839d7967c94a9ff9baecb1c3cf818a13ce36537b4beac65ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
